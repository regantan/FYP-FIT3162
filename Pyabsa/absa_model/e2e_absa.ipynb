{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-23 00:28:07] (2.4.1.post1) ********** \u001b[32mAvailable ATEPC model checkpoints for Version:2.4.1.post1 (this version)\u001b[0m **********\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) ********** \u001b[32mAvailable ATEPC model checkpoints for Version:2.4.1.post1 (this version)\u001b[0m **********\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) \u001b[32mDownloading checkpoint:multilingual \u001b[0m\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) \u001b[31mNotice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\u001b[0m\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) Load aspect extractor from checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) config: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.config\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) state_dict: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.state_dict\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) model: None\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) tokenizer: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.tokenizer\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) Set Model Device: cuda:0\n",
      "[2024-03-23 00:28:07] (2.4.1.post1) Device Name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\regan\\Documents\\FYP-FIT3162\\myenv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-23 00:28:13] (2.4.1.post1) The results of aspect term extraction have been saved in c:\\Users\\regan\\Documents\\FYP-FIT3162\\Pyabsa\\absa_model\\Aspect Term Extraction and Polarity Classification.FAST_LCF_ATEPC.result.json\n",
      "[2024-03-23 00:28:13] (2.4.1.post1) Example 0: But the \u001b[32m<staff:Positive Confidence:0.998>\u001b[0m was so nice to us .\n",
      "[2024-03-23 00:28:14] (2.4.1.post1) The results of aspect term extraction have been saved in c:\\Users\\regan\\Documents\\FYP-FIT3162\\Pyabsa\\absa_model\\Aspect Term Extraction and Polarity Classification.FAST_LCF_ATEPC.result.json\n",
      "[2024-03-23 00:28:14] (2.4.1.post1) Example 0: But the \u001b[31m<staff:Negative Confidence:0.9947>\u001b[0m and the \u001b[31m<food:Negative Confidence:0.995>\u001b[0m was so horrible to us .\n"
     ]
    }
   ],
   "source": [
    "from pyabsa import AspectTermExtraction as ATE\n",
    "aspect_extractor = ATE.AspectExtractor(\n",
    "\"multilingual\",\n",
    "data_num=100,\n",
    ")\n",
    "# simple inference\n",
    "examples = [\n",
    "\"But the staff was so nice to us .\",\n",
    "\"But the staff and the food was so horrible to us .\",\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "  result = aspect_extractor.predict(\n",
    "    text=ex,\n",
    "    print_result=True, \n",
    "    ignore_error=True,\n",
    "    eval_batch_size=32,\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
